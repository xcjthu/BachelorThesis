% !TeX root = ../main.tex

\chapter{实验}
\label{cha:experiment}

本章节主要介绍模型实现的具体细节以及实验结果。本文在大规模的文档级关系抽取数据集上进行了测试，并针对于模型的各个预训练任务进行了消融实验，分析模型效果提升的关键因素。

\section{数据集及评价指标}

\begin{table}[]
\centering
\begin{tabular}{lcccc}
\toprule
类型             & \# Doc. & \# Rel & \# Inst. & \# Fact    \\
\midrule
训练集 			& 3，035   & 96     & 38,269   & 34,715     \\
验证集           & 1，000   & 96     & 12,332   & 11,790     \\
测试集           & 1，000   & 96     & 12,842   & 12,101     \\
远程监督集        & 101,873  & 96     &1,508,320 & 881,298    \\
\bottomrule
\end{tabular}
\caption{数据集的划分}
\label{table:experiment:docred_stat}
\end{table}

正如章节~\ref{cha:related_work}中所介绍，有许多学者相继提出了许多文档级别关系抽取的数据集。Quirk et al.\cite{quirk2017distant}和 Peng et al.\cite{peng2017cross}从PubMed上获取了大量的无标注的生物医学文献，并利用远程监督的机制对文献进行关系标注。这两个数据集关注于生物医学领域的关系知识，并且为了避免文档级别远程监督带来过多的错误标注样例，它们将文本限定在三个句子以内，且测试集也为远程监督数据，不利于模型的测试。Li et al.提出了一个包含有 $1,500$ 篇文档的手工标注的文档级别关系抽取数据集，但是该数据集只包含一种关系类型，无法体现模型的普适性。

因此，本文选择在数据集DocRED上验证模型的效果。DocRED是目前人工标注的最大的文档级关系抽取数据集。该数据集收集了大量来自于Wikipedia的文档，并利用命名实体识别工具将所有文档中的命名实体抽取出来，并将这些实体与Wikidata中的实体进行对齐，接着利用Wikidata中的关系事实标注出了大规模的文档级远程监督数据集。接着，从远程监督数据集中挑选出部分文档进行手工标注，分步标注出了命名实体、实体类别及实体之间的关系。DocRED中包含的文档在主题上具有很高的覆盖率。同时，实体类型也包含了人物、地点、组织、时间、数字五种常见类型以及其他。关系类型涵盖面也非常广，包含有科学、艺术、人物生活等方面的 $96$ 种类型。经过分析，该数据集要求模型具有模式识别、逻辑推理、指代消解和常识推理等能力。因此，该数据集能够很好地评测本文提出的模型的效果。在本文中，我使用远程监督集合进行模型预训练，利用手工标注数据进行模型的微调和测试。

在本节中，我使用了 $F_1$ 和 $IgnF_1$ 作为评价指标。其中 $IgnF_1$ 在\citet{yao2019docred}中定义。因为训练集与验证集和测试集包含的关系事实之间具有交集，为了避免模型通过记忆实体对的名称来进行关系的预测，便提出了 $IgnF_1$ 这个指标。该指标衡量的是在将验证集和测试集中包含的训练集中有的关系事实删除之后，模型的效果，该指标更能够反映出模型理解文本、理解关系的能力。


\section{基线模型}
本文挑选了以下基线模型与本文的模型进行对比。
\begin{itemize}
	\item \textbf{CNN/LSTM/BiLSTM}。这一类模型代表的是传统的序列模型，即将文档当做句子的扩展进行处理。这一类模型将词向量、位置向量、实体指代向量、实体类型向量、距离向量作为特征，并将这些特征全部拼接在一起，作为模型编码器的输入。之后这类模型将输入编码成一个隐向量序列。文档中每个实体提及的用该提及对应单词的隐向量的平均值来表示，而每个实体由该实体对应的提及的向量的平均来表示。最后，对于每一个实体对，用一个双线性层进行编码，得到该实体对对应的关系概率。
	\item \textbf{ContextAware}。与上述模型不同，该模型考虑一篇文章中具有多个实体对的情况，每个实体对的表示应该与其上下文的实体对表示有关。该模型利用双向长短时记忆网络编码文档，并同样利用双线性层获取每个实体对的表示。紧接着，模型利用注意力机制，将目标实体对作为询问值，将上下文中包含的其他实体对作为键值，计算权重。目标实体对的最终表示为上下文实体表示的加权平均。该模型考虑了多实体推理的情况，但是在文档级别关系抽取中，大量实体对是没有关系的，因此在加权平均时会引入较多噪音，所以在本文场景下，该模型并没有太多优势。
	\item \textbf{BERT}。预训练语言模型在自然语言处理领域的许多任务上都取得了很大的成功。因此，本文也采用BERT作为基线模型之一，与主模型进行对比。与章节~\ref{cha:model}中介绍的相同，该模型使用了实体标签来在模型中引入指代信息。文章中每一个实体提及的表示为该实体提及对应的实体开始标签的隐向量，并利用最大池化层来获取实体表示，最后利用双线性层获得关系表示。
	\item \textbf{BERT-Two-Step}。该模型基于BERT进行改造。该模型提出大量的负样例造成了标签极度不均的问题，该问题将导致模型训练出现偏差。因此，该模型提出利用两步策略进行预测。在获得了关系表示之后，首先判断该实体对之间是否有关系，若有关系，则在第二步中预测具体的关系种类。但是该模型并没有能够很好解决标签不均衡问题，因此它将标签不均衡问题集中在第一步的判断上，而导致第一步判断准确率并不是很高，因此相比于BERT模型，该模型并没有明显提升。
	\item \textbf{HIN-BERT}。该模型认为，文档级关系表示涉及非常复杂的推理过程，而推理应该在实体级、句子级、文档级三个级别依次进行。因此，该模型提出使用层次化的门控循环神经网络依次对实体、句子、文档进行编码，并在三个级别分别使用推理层进行信息融合。最后，模型通过结合三个层次的输出来预测实体对最终的关系。
\end{itemize}

这些模型都是在手工标注数据集上进行训练的，没有用到远程监督数据集中的信息。因此，为了充分对比基线模型与本文提出的主模型，本节将不仅给出上述模型在手工标注数据集上训练得到的结果，也将给出在远程监督数据集上训练得到的结果。从而，全面地验证本文提出的主模型的有效性。


\section{模型实现细节}

\smallskip
\noindent
\textbf{预训练参数设置。} 本文提出的模型基于BERT-base进行预训练。在预训练时，学习率设置为 $3 \times 10^{-5}$，在精度 FP16 下进行预训练。预训练时，batch size设置为 $16$。经过双线性层得到的关系的表示的维度设置为 $256$。对远程监督集合进行降噪时，本文保留了每一篇文章中得分最高的 $20$ 个实体对，其余实体对被认为是噪音丢弃了。并且对于预训练模型，我训练 $1000$ 个batch，即完成一个 epoch 的训练，总共训练了 9 个 epoch。

\smallskip
\noindent
\textbf{微调参数设置。} 在对模型进行微调时，学习率设置为 $1 \times 10^{-5}$，在精度 FP32 下进行微调。同时，微调时， batch size设置为 $4$。在对手工标注数据进行降噪时，为了让降噪后保留足够多的正样例，同时筛除足够多的负样例，本文保留了每一篇文章中得分最高的 $2 \times N_{\text{ent}}$，其中 $N_{\text{ent}}$ 是该文章中包含的实体数量。本文认为，实体数量越多则文章中包含的关系事实越多。微调时，所有结果均是在训练了不超过 $30$ 个epoch得到的。

\section{实验结果}

\begin{table}
\centering
\begin{tabular}{lcccc}
\toprule
             & \multicolumn{2}{c}{Dev} & \multicolumn{2}{c}{Test} \\
Model        & $F_1$      & $IgnF_1$   & $F_1$      & $IgnF_1$      \\ \midrule
\multicolumn{5}{c}{有监督场景} \\ \midrule
CNN*          & 43.45      & 41.58      & 42.26      & 40.33       \\
LSTM*         & 48.44      & 50.68      & 47.71      & 50.07       \\
BiLSTM*       & 50.94      & 48.87      & 51.06      & 48.78       \\
ContextAware* & 51.09      & 48.94      & 50.70      & 48.40       \\
BERT          & 55.67      & 53.32      & 56.17      & 53.66          \\
BERT-Two-Step $\clubsuit$ & 54.42      & --         & 53.92      & --          \\
HIN-BERT $\spadesuit$      & 56.31      & 54.29      & 55.60      & 53.70       \\
\midrule
\multicolumn{5}{c}{远程监督场景} \\ \midrule
CNN*          & 42.76      & 33.24      & 42.00      & 32.33       \\
LSTM*         & 49.92      & 39.37      & 48.88      & 38.27       \\
BiLSTM*       & 51.72      & 41.44      & 49.80      & 39.15       \\
ContextAware* & 51.39      & 40.47      & 50.12      & 39.16       \\
\midrule
BERT+D        & 57.42      & 55.88      & 57.20      & 55.53       \\
BERT+D+P    & \textbf{58.65}  & \textbf{57.00}  & \textbf{58.43}  & \textbf{56.68} \\ 
%Best in LeadBoard   & --      & --     & 62.30      & 60.10       \\ 
\bottomrule
\end{tabular}
\caption{主要的实验结果。其中标有*，$\clubsuit$ 和 $\spadesuit$ 的实验结果，分别是从 \citet{yao2019docred}，\citet{wang2019fine}和\citet{tang2020hin}中获取的。其中基线模型给了有监督场景和远程监督场景两种结果。}
\label{main_result}
\end{table}

本文主要的实验结果如表格~\ref{main_result}中所示。其中 $D$ 表示降噪模块，$P$ 表示预训练。表格中分别给出了只进行降噪不预训练的模型效果，和既进行降噪又进行预训练的效果。观察表格中实验结果，我们可以得出以下结论：
\begin{itemize}
	\item 本文提出的模型显著优于所有的基线模型。在测试集中，相比于目前最好的基线模型HIN-BERT，本文提出的模型在 $F_1$ 和 $IgnF_1$ 两个指标上分别提升了 $2.83$ 和 $2.98$个点。这说明本文提出的降噪模块及三个预训练任务能够充分利用大规模远程监督数据上的信息，关于具体每一个预训练任务起到了多大的作用，将在之后的消融实验中进行讲述。
	\item 即使是不进行预训练的模型的结果（Bert+D）也能够超过最好的基线模型。这说明在文档级关系抽取中，太多的NA数据，确实给模型训练以及预测带来了较大的困难。这也说明了本文提出的降噪机制可以使模型效果有一个很明显的提升。该机制与具体模型无关，可以运用到各种各样的模型当中去。相比于对于NA数据有特殊处理的BERT-Two-Step模型，本文提出的降噪机制提升非常明显，进一步证明了模型降噪机制的有效。
	\item 在远程监督场景下训练的模型与在有监督场景下训练的相同模型相比，远程监督场景下训练的模型的效果相对来说都要差一些。这与句子级别远程监督实验不符。这说明在文档级别关系抽取中，远程监督机制引入了更多数据的同时，带来了远比句子级别要多的错误标注样例。这样的错误标注给模型训练带来了很大的偏差，从而使得模型效果下降。
	\item 本文提出的模型通过预训练的模式来利用远程监督数据。相比于远程监督场景下的基线模型，本文提出的模型效果是能够提升的。说明本文提出的三个预训练任务是十分有效的，能够在滤除噪音的同时，充分捕捉海量数据中的信息。
	\item 本文实现的基线模型BERT效果优于BERT-Two-Step模型，同时与HIN-BERT模型也能够达到类似的效果。这说明在BERT模型中引入实体标记，确实可以给模型带来指代信息，帮助模型更好地构建实体的表示，从而得到更具信息量的关系表示。
\end{itemize}

主实验的结果充分证明了本文提出模型的有效性，接下来为了分析各个预训练任务给模型带来的增益，本文继续开展了消融实验。


\section{消融实验}

\begin{table}
\centering
\begin{tabular}{lcccc}
\toprule
             & \multicolumn{2}{c}{Dev} & \multicolumn{2}{c}{Test} \\
模型        & $F_1$      & $IgnF_1$   & $F_1$      & $IgnF_1$      \\ \midrule
our model   & 58.65  & \textbf{57.00}  & \textbf{58.43}  & \textbf{56.68} \\
\midrule
\quad w/o M    & 58.39      & 56.76      & 57.60      & 55.81            \\
\quad w/o S    & 58.48      & 56.73      & 58.13      & 56.30            \\
\quad w/o N    & 57.19      & 55.61      & 56.71      & 54.94            \\ 
\midrule
\quad w/o Inter & \textbf{58.68}  & 56.96      & 57.72      & 55.87             \\
\quad w/o Intra & 57.78      & 56.18      & 57.62      & 55.89              \\
\bottomrule
\end{tabular}
\caption{在DocRED数据集上消融实验的结果。}
\label{ablation_study}
\end{table}

本文开展了一系列的消融实验来验证不同预训练任务的有效性。消融实验的实验结果如表格~\ref{ablation_study}中所示。其中我们将预训练任务一个一个从模型上移除，并观察移除之后模型的效果。其中 \textbf{w/o M}，\textbf{w/o S}，\textbf{w/o N}分别指的是模型移除了实体匹配、关系事实对齐、关系检测的结果。从消融实验的结果中可以得出以下结论：
\begin{itemize}
	\item 每一个预训练任务对应模型效果都是有帮助的。因为，从实验结果可以看出，删除任何一个任务，模型效果都会下降。
	\item 任务关系检测对于模型效果影响最大。从实验结果可以看出，当移除了关系检测这个任务之后，模型效果有了一个比较大幅的下降。甚至于，没有了关系检测任务之后的模型效果比没有预训练的模型（BERT+D）效果还要差。该任务旨在帮助模型学会区分正样例和负样例，而剩余两个预训练任务实体匹配和关系事实对齐，不涉及对于负样例的建模。因此，在没有关系检测任务的情况下，模型会将负样例和正样例混在一起，从而导致了效果的下降。
	\item 任务关系事实对齐对于模型效果影响最小。这说明其实区分不同的关系对于模型来说并不特别困难。
\end{itemize}

 \hspace*{\fill}
 
不仅如此，本文提出的实体匹配和关系对齐两个任务都包含跨文档和文档内两个部分的子任务，而关系事实对齐只包含跨文档任务。因此，为了探索跨文档和文档内两类子任务对于模型效果的影响，本文进一步开展了消融实验。其中 \textbf{w/o Inter} 是指移除了跨文档子任务后模型的效果，\textbf{w/o Intra} 是指移除了文档内子任务后模型的效果。从实验结果中，可以得出以下结论：
\begin{itemize}
	\item 删除了跨文档子任务之后，模型在验证集上的效果并没有明显的下降，甚至在 $F_1$ 指标上略高于主模型，但是在测试集上，模型效果就相对来说要差很多。这说明，没有了跨文档子任务之后，模型效果并不鲁棒，对于不同的测试集合来说，效果无法做到比较稳定，有比较大的起伏。
	\item 文档内子任务的删除对于模型效果有较大影响，因为一篇文档中会有多个实体、多个关系事实，模型在编码时需要同时考虑这些内容才能够获得更好的表示。当去除了文档内子任务时，模型将不会同时优化多个关系事实的表示，而是只尽量优化被挑选出的关系事实的表示。因此，模型效果出现下降。
\end{itemize}


\hspace*{\fill}
 
本节中，我使用了DocRED数据集来验证模型的有效性，模型利用DocRED的远程监督数据进行预训练，利用DocRED手工标注数据进行微调及测试。本文使用了 $F_1$ 和 $IgnF_1$ 作为评价指标。主实验表明，本文提出的模型可以超过所有基线模型，实现一个较好的效果。同时，本文还进行了消融实验验证了三个不同预训练任务的有效性，其中关系检测任务对于模型效果影响最大。紧接着，本文还对跨文档子任务和文档内子任务对模型效果的影响进行了研究。跨文档子任务可以帮助模型提升在不同测试集上的鲁棒性，文档内子任务可以帮助模型同时建模一篇文档内的多个关系事实。










