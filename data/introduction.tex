% !TeX root = ../main.tex

\chapter{研究背景}
\label{cha:intro}

关系抽取（Relation Extraction，RE）旨在判断给定文本中实体之间的关系。该任务是为了能够实现知识图谱的自动构建而设计。知识图谱将具体的事物表示为实体，将实体与实体之间联系表示为关系，能够为下游任务提供丰富的世界知识、常识知识。知识图谱在很多领域有着非常广泛的应用，例如知识问答、信息检索等领域。目前也已经有很多开源的大规模的知识图谱，例如WikiData\footnote{https://www.wikidata.org/}、Freebase等。但是知识图谱的构建是非常耗费人力物力的，因此学者们关注于如何实现自动构建大规模的知识图谱。

关系抽取任务也就应运而生。随着深度学习的快速发展，很多神经网络关系抽取模型已经在多个数据集上被验证是有效的，并且取得了非常不错的效果。但是这些模型大多都是为了句子级别关系抽取任务设计的\cite{zeng2014relation,lin2016neural,zeng2015distant}，旨在抽取出句内的关系。然而，句子级别关系抽取是一个非常受限的场景，在实际应用中覆盖的关系数量较少。因为，有非常多的关系是句间关系，需要综合多个句子的信息才能够抽取出来。因此，文档级别关系抽取（Document-level Relation Extraction，DocRE）被提出来，在近几年也受到众多学者的广泛关注。本文提出了一种预训练机制，能够帮助模型充分利用大规模的远程监督数据，实验结果也表明本文提出的模型超过了以往文档级别关系抽取模型，实现了效果的大幅提升。本节将围绕文档级关系抽取及预训练模型讲述本文的研究背景。

\section{文档级关系抽取}

\begin{figure}[ht]
	\centering
	\includegraphics[width = \linewidth]{figures/model/example1}
	\caption{一个文档级关系抽取的例子。给定一篇文档及其中包含的多个实体，模型需要从中理解文本，从文本中抽取出所有可能的关系事实。}
	\label{intro:example}
\end{figure}

文档级关系抽取旨在理解文档，从文档中同时抽取出句内及句间关系。近期，众多学者关注到文档级关系抽取任务，并提出了相应的数据集及模型\cite{christopoulou2018walk,sahu2019inter,yao2019docred}。图~\ref{intro:example}展示了一个文档级关系抽取的样例。给定了一篇文档及其中包含的多个实体之后，模型需要理解文档内容，并进行多实体、多句的推理。如图中所示，模型不仅需要抽取出句内关系，也要根据语义推断出句间关系: (\texttt{St. Vincent-St. Mary High School}, locate\_in, \texttt{Akron})。目前已经有许多工作聚焦于文档级关系抽取这一任务，有学者尝试利用手工设计的特征\cite{gu2016chemical,gu2017chemical}来对多实体、多句子进行建模；或者利用图神经网络来实现跨句、跨实体的信息传递\cite{christopoulou2019connecting,sahu2019inter}。这些工作都依赖于高质量的手工标注数据，而忽略了大规模的弱监督数据。

事实上，在句子级别关系抽取中已经有了大量工作尝试使用大规模的远程监督数据进行训练，并且取得了非常不错的效果\cite{lin2016neural,zeng2015distant}。但是远程监督机制将不可避免地在数据中引入的错误标注，尤其在文档级别这个问题更加明显。因此，尚未有工作能够有效地利用大规模的文档级别远程监督数据。本文创新性地提出使用预训练的机制来利用大规模的远程监督数据。下小节将介绍自然语言处理中的预训练模型。

\section{预训练模型}
在自然语言处理领域，已经有很多预训练模型\cite{devlin2019bert,yang2019xlnet,joshi2020spanbert}被提出，这也证明了利用大规模的无监督的文本能够很好的提高模型理解文本的能力，并且能够有效地提升模型在下游任务的效果。这些模型主要通过预训练语言模型来从大规模无监督的文本中学习语言知识，并通过将语言知识迁移到下游任务中从而取得模型效果的提升。预训练语言模型通过遮盖句子中某个词，并要求模型还原出被遮盖的词语的方式来进行训练。例如，给定句子：\texttt{There is a fish swimming in the [MASK].}，模型需要还原出 \texttt{[MASK]}代表的词语。通过这种方式，模型能够有效利用大规模的无监督数据，提升模型理解文本的能力。之后也有工作相继提出能力更强的预训练模型，例如融入知识的预训练语言模型\cite{zhang2019ernie}，能够预测短语的预训练语言模型\cite{joshi2020spanbert}等。接着也有许多学者针对于特定的任务开展模型的预训练，例如针对于文本生成的预训练模型\cite{lewis2019bart,song2019mass,dong2019unified}，针对于推荐系统的预训练模型\cite{yuan2020parameter}，面向问答系统和检索系统的预训练模型\cite{guu2020realm}。这些工作都充分证明了大规模无标注文本中蕴含有丰富的知识，通过预训练的方式可以将这些知识融入到模型参数当中，并迁移至下游任务，达到更好的文本理解能力。

因此，受以上工作启发，本文针对于面向文档级关系抽取的预训练展开了研究工作。




\section{面向关系抽取的预训练}

因此，本文旨在充分利用大规模的弱监督的文本，来提升模型理解文本的能力，进而使得模型能够获得更好的文档级关系抽取的能力。其中弱监督（远程监督）机制由\citet{mintz2009distant}提出，传统手工构建数据集是非常耗费人力物力的，该机制可以通过将文本中出现的实体与知识库的实体进行对齐，并认为出现在两个有关系的实体出现一个句子中，那么该句子就表达了这两个实体的关系。因此，该机制可以自动构建大规模的语料库，但该机制的假设并不完全正确，因此该机制也会不可避免地在数据中引入大量错误标注的噪音数据。

之前也有一些工作尝试从文本中学习可迁移的文本表示。\citet{wu2019open}和\citet{gao2019neural}利用孪生网络，从有监督的文本中学习关系表示，并将学习到的知识迁移到无监督的文本中进行运用，但是这两个工作依赖于大规模的有标注文本。\citet{soares2019matching}则首次尝试使用预训练的方式利用句子级别弱监督数据，但是该模型由于受噪声影响过大，而无法被应用至文档级别。

考虑到之前在句子级别弱监督数据上开展的工作\cite{lin2016neural,zeng2015distant}会不同程度地受到噪音影响，在文档级关系抽取中利用大规模的弱监督数据是非常具有挑战性的。因为将远程监督机制的应用范围从一个句子简单扩展到一篇文档时，相比于句子级别，该机制引入的噪音甚至将成倍增加。因此，为了能够更好地利用这部分远程监督数据，本文提出了一个通过三种特殊任务训练的预训练模型：
\begin{itemize}
	\item 第一个任务是针对于实体和实体提及的。该任务被分成文档内子任务和跨文档子任务。文档内子任务遮盖实体提及，并要求模型预测出该实体提及属于该文档中哪一个实体。通过这个任务模型可以掌握文档内部实体指代的信息，从而促进信息跨实体流动。跨文档子任务的目的则是让出现在不同文档内的相同实体具有相类似的表示方式。该子任务要求模型匹配不同文档中的相同实体，从而使得模型能够掌握跨文档的实体信息之间的关联。
	\item 第二个任务主要针对于关系层面。具体来说，本文任务不同文档中出现的相同的关系事实，即相同实体对，它们的关系表示是相似的。因此，在该任务中，给定两篇文档中所有的关系表示，模型需要将这两篇文档中表示了相同的关系事实的表示进行对齐。这个任务可以帮助模型掌握跨文档的关系事实之间的关联。
	\item 第三个任务主要针对于```Not-A-Relation(NA)```关系进行设计。在文档级关系抽取中，NA关系占了绝大部分。根据数据显示，文档级数据中有超过 $96.8\%$ 的实体对之间都是不存在关系的\cite{yao2019docred}。因此，本文设计的第三个任务主要集中在NA关系的判断上。该任务目的是帮助模型从大量的样例中，检测出非NA样例，即有关系的正样例。不仅如此，本文还利用该任务训练了一个降噪模块，从文档中滤除大量的NA样例，从而使得模型训练时不会因标签过度不均衡而训练出现偏差。
\end{itemize}

第一个和第二个任务主要帮助模型学习到从文本中筛选信息生成好的实体表示和关系表示。第三个任务则主要帮助模型处理文档级关系抽取中特有的NA样例过多的情况，并且帮助模型减弱弱监督数据中不可避免的错误标注问题带来的负面作用。基于上述三个预训练任务，模型可以从大规模弱监督数据中充分学习到有用的信息，并将信息编码至实体和关系的表述当中。

在实验中，本文在大规模的文档级关系抽取数据集上验证了模型的有效性，并且模型实现了效果上的显著提升。同时，本文还对实验结果进行了详尽的分析，并开展了消融实验，充分验证了模型的有效性，并且充分证明了大规模远程监督数据对于文档级关系抽取任务的重要性。据我所知，本文是第一个针对于文档级关系抽取任务提出的预训练模型。为了推动领域发展，我将代码开源至 \url{https://github.com/xcjthu/DocRED}。

\hspace*{\fill}

总结而言，本文具有以下贡献：
\begin{itemize}
	\item 本文提出了三个特殊的预训练任务，能够很好地帮助模型利用大规模的弱监督数据，实现在文档级关系抽取任务上效果的显著提升。本文是第一个利用大规模弱监督数据进行预训练的文档级关系抽取模型。
	\item 本文基于上述预训练任务提出了对于文档级关系抽取任务的降噪的框架，该框架具有普适性，能够被利用至其他文档级模型当中，减少NA数据带来的影响。
	\item 本文开展了详尽的实验，证明了模型的有效性，以及利用大规模弱监督数据的必要性。
\end{itemize}

接下来本文将从相关工作、具体方法、实验以及结论几个方面来详细介绍本文的工作。









